# MEP Assistant (johnE.ai prototype)

## üöÄ Quick Start

1. **Requirements**
    - Node.js
    - Python 3.8+
    - Ollama (local LLMs, e.g., `llama3:8b`)

2. **Install backend dependencies:**
    ```sh
    pip install -r requirements.txt
    ```

3. **Install frontend dependencies:**
    ```sh
    npm install
    ```

4. **Run Ollama & load the model**
    - `ollama serve`
    - `ollama run llama3:8b`

5. **Run the backend:**
    ```sh
    python mep_api.py
    ```

6. **Run the Electron app:**
    ```sh
    npm start
    ```

## ‚ú® More info
- See [Ollama docs](https://ollama.com/) for model help.
- For issues, DM [your contact].

---

### **TL;DR:**
- **Yes, it‚Äôs just a zip and instructions** right now.
- That‚Äôs fine for technical/early users.
- To make it even easier, ‚Äúbuild‚Äù your Electron app into an installer in the future!

---

If you want to take the next step (packaging as a real installer, or even bundling the backend!), **just say the word**‚ÄîI‚Äôll give you a step-by-step guide for that.

**Let me know if you want:**
- a sample README
- packaging/installer instructions
- advice on what to add next!
